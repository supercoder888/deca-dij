\documentclass[final,5p,times,twocolumn,authoryear]{elsarticle}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{listings}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}

\algnewcommand\algorithmicinput{\textbf{INPUT:}}
\algnewcommand\INPUT{\item[\algorithmicinput]}

\algnewcommand\algorithmicreturns{\textbf{RETURNS:}}
\algnewcommand\RETURNS{\item[\algorithmicreturns]}

\algnewcommand\algorithmicvars{\textbf{VARIABLES:}}
\algnewcommand\VARIABLES{\item[\algorithmicvars]}

\algnewcommand\algorithmicconsts{\textbf{CONSTANTS:}}
\algnewcommand\CONSTANTS{\item[\algorithmicconsts]}

\newcommand{\var}[1]{\mathit{#1}}
\newcommand{\func}[1]{\mathrm{#1}}

\DeclareMathOperator*{\argmax}{\arg\!\max}

\def\bibsection{\section*{References}}

\begin{document}
\begin{frontmatter}

\title{Decision-theoretic file carving}

\author[add1]{Pavel Gladyshev}
\ead{pavel.gladyshev@ucd.ie}
\author[add2]{Joshua Isaac James}
\address[add1]{Digital Forensics Investigation Research Laboratory, University College Dublin, Belfield, Dublin 4, Ireland}
\address[add2]{Digital Forensics Investigation Research Laboratory, Hallym University, 1 Hallimdaehak-gil, Chuncheon-si, Gangwon-do, South Korea}
\begin{abstract}
This article explores a novel approach to file carving by viewing it as a decision problem. It allows us to design algorithms that produce best effort results under the given resource constraints. It is important for digital forensic triage as well as for e-discovery. As an illustration, we develop a novel file carving algorithm together with a novel linear-time detector of JPEG data and examine its performance.
\end{abstract}

\begin{keyword}
decision-theoretic file carving \sep 
file carving \sep 
digital forensic investigation \sep 
Digital forensic triage \sep
preliminary analysis
\end{keyword}
\end{frontmatter}

\section{Introduction}

Digital forensic investigations must process large amounts of evidential data with limited resources \citep{Casey2009, pollitt2013triage}. Modern solutions to the data processing problem combine advances in automatic detection of relevant data with some form of selective human exploration to identify sample data and to validate output results \citep{marturana2013machine, schell2007cyber, james2014measuring}. Despite significant time and cost savings that automation can bring to digital investigations, this approach still relies on exhaustive processing of the suspect data.

Various methods have been proposed that attempt to prioritise exhibits by first prioritising suspect data sources that the exhibit contains \citep{shaw2013practical, rogers2003role, overill2013triage}. Several works \citep{Koopmans2013,Casey2009} have discussed the idea of digital forensic triage; an investigation processes with the goal of not always requiring an exhaustive search of all devices, -and helps with decisions of relevancy and prioritisation.

There are many situations where a digital investigator is limited either in time or available computational power. For example, a parole officer may use a portable forensic tool to periodically check that an offender's computer does not contain child abuse material. The time and processing power available to the officer on site is limited and -- given the growth in data storage capacity -- may not be sufficient to exhaustively explore the contents of an offender's computer. Thus a solution is required that would combine automatic processing with probabilistic sampling and prioritisation aimed at discovering relevant information more quickly.

This paper explores the idea of probabilistic sampling and prioritisation in the context of file carving. It shows that when the investigator is looking for files of a particular kind, such as large JPEG files, this information can be used to speed up file carving by prioritizing processing of data blocks that are most likely to contain relevant data. This is demonstrated by constructing specialised file carver for digital photographs that outperforms\footnote{The relevant measure of performance is discussed in Section \ref{utility-of-carving}} traditional file carvers.

\subsection{Contribution}
This work contributes to the field of digital forensic investigation by proposing a \emph{best-effort} file carving method that aims to recover as much relevant information as possible under the given time- and processing power constraints. It is suitable for digital forensic triage purposes as defined by \citep{Koopmans2013}. Specifically, this work provides:
\begin{itemize}
	\item{a formal statement of file carving as decision problem;}
	\item{an illustration of how simplifies mathematical models and simulation can help improving file carving efficiency;}
	\item{a JPEG image carving algorithm and software application that could recover potentially relevant images (photographs) faster than traditional carvers.}
\end{itemize}

\section{Sequential file carving}

File carving is often used in digital investigations. Traditional file carving programs, like {\tt foremost} by \citep{richard2005scalpel} or {\tt photorec} by \citep{grenier2007photorec} can be described as \emph{sequential} file carvers. The basic sequential carving algorithm works as follows: the carving starts at the first data block of the drive and progresses consecutively until the end of the drive is reached. Each block is checked against a database of header and footer signatures. If the block matches some \emph{header} signature, it is assumed to be the first block of a file to be recovered. The rest of the file is assumed to occupy the subsequent consecutive blocks up to and including the matching ``footer'' block. If the ``footer'' block cannot be found, the length of the recovered file is capped at a predefined limit. 

Sequential file carving works because the mainstream file systems are designed to store file data in consecutive disk blocks whenever possible, in order to maximize performance of mechanical hard disk drives. While file fragmentation does occur, \citep{garfinkel2007carving} shows that the majority of files in personal computers are not fragmented.  

Prior research in file carving mainly focused on improving accuracy; more accurate techniques for detecting file fragments of particular kinds were proposed\footnote{For more detail see \citep{Veenman2007statistical}, \citep{li2011novel}, and \citep{Garfinkel2015hashbased}}, and algorithms for detection and reassembly of fragmented files were developed that achieve quadratic time complexity\footnote{$O(n^2)$ time complexity where $n$ is the number of fragments to be reassembled. For more detail see \citep{memon2006automated}}. 

The performance of the basic sequential carving algorithm was improved upon by Richard, et al \citep{Richard2007inplace}, and similarly Meijer \citep{MeijerRob2012}, who proposed in-place\footnote{Also known as zero-storage} file carving in an attempt to lower the time and space requirements compared to traditional sequential carving. These approaches identify the challenges with traditional file carving, and propose methods to reduce carving resource requirements by minimizing the amount of disk I/O associated with copying recovered file data. Essentially, once file data is discovered, pointers are created in virtual file systems that point to the data directly on the suspect disk rather than copying data from the suspect disk. However, the approach used for \emph{finding} the file data is still sequential, as before.

\section{File carving as a Battleships game}

File carving is a process of exploration, and it is instructive to consider file carving as an analogy of the Battleships\footnote{The Battleships game is played as follows: each player has a pad of paper with two 100-square grids (maps) -- one for arranging the player's ships and the other for noting the discovered locations of the opponent's ships. Before the start of the game, each player secretly arranges their ships on their grid. A ship occupies a number of consecutive squares and is oriented either vertically or horizontally. The ships cannot overlap. The game proceeds in turns with each player sending a "bomb" to a particular square on the enemy grid. The other player then responds whether the "bombing" succeeded in damaging one of their ships. A ship is destroyed when all of the squares it occupies are hit by the opponent. The game ends when one of the players completely destroys the other player's fleet.} game, in which one player -- the data storage -- arranges her ships (the relevant files) on a ``canal'' and the other player -- the file carver -- tries to destroy the opponent's ships with the minimal number of bombs (see Fig. \ref{fig:battleships_on_a_canal}).  

The simplest strategy that is guaranteed to destroy the opponent's fleet is to bomb all squares consecutively from left to right, which is precisely what sequential carvers do. Although effective, this strategy is not necessarily the most efficient. A human players of Battleships would randomly bomb the opponent's map until one of the ships is hit. After that, she would bomb the adjacent squares until the damaged ship is completely destroyed. It is intuitively plausible that similar approach could be used in file carving and that it may outperform sequential carving algorithms. In the rest of this paper we present theoretical and experimental results that validate this intuition using ecision theory, numeric simulation, and actual file carving experiments.

\begin{figure*}
  \centerline{\includegraphics[width=0.6\textwidth]{fig3}}
  \caption{A 1-dimensional Battleships game.}
  \label{fig:battleships_on_a_canal}
\end{figure*}

\subsection{Decision theory basics}

Decision theory is a branch of mathematics that studies decision making. In its basic form, it represents decision as a choice between several alternative actions that together form the set $A$ (possible actions): 

\begin{equation}
A=\left\{a_1,a_2,\dots,a_n\right\}
\end{equation}

Any chosen action $a_i \in A$ can result in one of several possible, mutually exclusive outcomes that together form the set $O$ (possible outcomes):  

\begin{equation}
O=\left\{o_1,o_2,\dots o_m \right\}
\end{equation}

The likelihood of a particular outcome $o_j \in O$ happening as a result of the action $a_i \in A$ is represented by the conditional probability $P(o_i \mid a_i)$. 

The desirability of a particular outcome is modelled by the \emph{utility} function $u()$ that assigns a real-valued score to each outcome. The higher the utility score, the more desirable is the outcome. The cost associated with doing a particular action can be modeled as a reduction of utility scores of the corresponding outcomes. To model it formally we define $u()$ as function of both the outcome \emph{and} the action:

\begin{equation}
u : O \times A \rightarrow \mathbb{R}
\end{equation} 

The principle of \emph{Maximum Expected Utility} (MEU) states that a rational decision maker should choose the action with the highest mathematical expectation of the utility score across all possible outcomes:

\begin{equation} \label{eq:decision}
a = \argmax_{a_i \in A}\sum_{o_j \in O} u(o_j,a_i)P(o_j \mid a_i )
\end{equation}

\subsection{File carving as decision problem}

Like a player of Battleships that chooses which square to bomb, file carver chooses which disk block to process next. Let array $B$ represent the drive subjected to file carving:

\begin{equation}
B=\left(b_0,b_1,\dots b_m \right)
\end{equation}

Each element $b_0 \dots b_m$ represents a separate data block on the drive. 

Let file carving \emph{action} be the act of processing a particular disk block $b_k \in B$. There are two possible outcomes: $b_k$ contains the relevant data ($r$), or $b_k$ does not contain relevant data ($n$):

\begin{equation}
O=\left\{r,n\right\}
\end{equation}

Given that hard disk drives have physical read-write heads, that may have to be repositioned to access a different disk block, it is convenient to \emph{refer} to a particular block by its \emph{offset} from the last processed block. We will use such offsets instead of block indices to identify actions that can be taken by the carver:

\begin{equation}
A=\left\{\dots,-3,-2,-1,1,2,3\dots\right\}
\end{equation}

For example, if the last processed block was $b_{10}$, the action $a=-3$ means examination of the block $b_7$. 

\subsubsection{Utility of file carving actions}
\label{utility-of-carving}

In triage situations it is important to get relevant files quickly. Sequential carving takes long time to find relevant files if they are spread throughout the disk space. Figure \ref{fig:sequential} shows a plot of the total amount of data recovered by a sequential carver as function of time. Steep climbs occur when the carver finds relevant data; long horisontal stretches occur when the carver scans through drive areas devoid of relevant data. Figure \ref{fig:sequential} clearly shows that utility of carving actions can be defined as the \emph{slope} of that curve:

\begin{equation}
\label{eqn:utility-def}
u(o,a)=\begin{cases} 
      0 & o = n \\
      \frac{1}{T_{proc}(a)} & o = r 
   \end{cases}
\end{equation}
where
\begin{equation}
T_{proc}(a) = t_{RAM} + t_{seek}(a)
\end{equation}

Definition (\ref{eqn:utility-def}) consists of two parts. The first part of it says that processing blocks containing irrelevant data has zero utility in the sense that it does not increase the total amount of recovered data. The second part states that the usefulness of processing a block of relevant data is determined by how fast we process it -- the slower we process it, the more gentle is the slope of the curve in Figure \ref{fig:sequential}, and lower the utility of it. 

$T_{proc}(a)$ is the time required to process a disk block. It consists of a relatively constant component $t_{RAM}$ -- the time required to read a block of data into RAM and process it there, and $t_{seek}(a)$ -- the drive seek time\footnote{the time taken for a disk drive to locate the area on the disk where the data to be read is stored.} that generally depends on how far the target data block is from the last processed block. If the block at the offset $a$ contains relevant data, its utility is the reciprocal of $T_{proc}(a)$.

\begin{figure*}
	\center
	\includegraphics[width=0.9\textwidth]{wc_fig}
	\caption{The total amount of data recovered by a sequential file carver as a function of time. In this example the relevant data (JPEG images) occupied approximately 5\% of a 32Gb disk image.}
	\label{fig:sequential}
\end{figure*}

The substitution of  (\ref{eqn:utility-def}) for $u(o,a)$ in (\ref{eq:decision}) produces the deicsion principle for file carving:

\begin{equation} \label{eq:carving-decision}
a = \argmax_{a \in A} \left( \frac{1}{T_{proc}(a)} p(a,l)\right)
\end{equation}

where function $p(a,l) = P(o=r \mid a,b_l)$ represents the probability of finding a relevant data block at a particular offset $a$ from the last processed block $b_l$. Parameter $l$ is the index of the last processed block: $0 \leq l \leq m$. Function $p(a,l)$ represents the current ``knowledge'' of the carver of where the relevant blocks are likely to be and serves as a scaling factor reducing the benefit of processing a particular block if that block is unlikely to contain relevant data.

To implement a carver based on the Equation (\ref{eq:carving-decision}), the following things are required:

\begin{enumerate}
\item{a reliable and efficient method to detect \emph{relevant} data blocks. Unlike the header and footer signatures used in sequential carving, this method should work for arbitrary blocks from the middle of the file;}
\item{an understanding of the shape and properties of $p(a)$ in the context of a particular carving task, and how $p(a)$ changes when some block is examined and found to be either relevant or irrelevant;}
\item{an efficient way to determine $\argmax_{a \in A} \left( \frac{1}{T_{proc}(a)} p(a,l)\right)$ at run time.}

\end{enumerate}

It is also important to understand and practically demonstrate the circumstances in which decision-theoretic file carver would outperform sequential file carver. To prove the viability of decision-theoretic carving we developed a specialised file carver for recovering large JPEG files. 

\section{The need for selective recovery of large JPEG files}

Suppose that parole officer needs to check the computer of a sex offender for presence of raw digital photographs that may indicate production of child abuse material. To do it, she or he needs a program that can quickly find as many raw digital photographs as possible. The computer in question may not have much processing power, and the time for analysis may be limited. Decision-theoretic approach fits this problem very well, because it focuses on making best decision at every step leading to the best-effort results under given circumstances.  

A digital camera sensor typically produces raw photographs at a fixed resolution. The raw sensor data is then resized and compressed according to the image quality settings specified by the user. The resulting JPEG file is, typically, several megabytes in size. 

Figure \ref{fig:jpeg-dist} shows a histogram of sizes of digital photographs from a personal collection of 5000 digital photographs accumulated over 10 years. There are several peaks corresponding to different cameras and image quality settings used over the years. Each peak resembles the ``bell curve'' of the normal distribution\footnote{Recall that JPEG encoding divides the image into square fragments $8 \times 8$ pixels. Every such fragment is compressed independently meaning that by the central limit theorem of probability theory, the probability distribution of JPEG sizes for a particular camera setting converges to normal (Gaussian) distribution.}. Note that the vast majority of JPEG files in Figure \ref{fig:jpeg-dist} are bigger than 700 Kb in size. The two right-most peaks in the histogram correspond to images produced by a newer camera, whose JPEG files a bigger than 2000 Kb. It is highly likely that mainstream camera resolution will continue to grow leading to larger size of JPEG files. 

\begin{figure*}
  \centerline{\includegraphics[width=0.9\textwidth]{jpeg-sizes}}
  \caption{Histogram of actual file sizes from a personal collection of raw digital photographs in JPEG format. Horizontal axis is the size of files in Kibibytes, vertical axis is the number of files. Each of the peaks visible in the histogram corresponds to a particular camera and image size and quality settings.}
  \label{fig:jpeg-dist}
\end{figure*}

\section{Detector of JPEG data}
\label{sec:detector}

A fast and accurate detector of relevant data is essential for efficient operation of decision-theoretic file carver. Our initial implementation of JPEG data detector used Support Vector Machine (SVM) classifier based on the ideas of Li \emph{et al.} described in \citep{li2011novel}. The detector used 256 data features corresponding to byte-value frequencies. The detector was trained using 17,918 training samples that contained 8,985 samples from known Non-JPEG files and 8,933 samples from known JPEG files. It included samples from all sections of JPEG files. After training the detector was tested on a much larger data sample derived from a different source. It demonstrated 99\% true positive rate and 33\% false positive rate. 

The high false positive rate is due to relatively high entropy of compressed JPEG data. According to our tests the trained linear SVM classifier did not distinguish JPEG data from other high-entropy data, such as ZIP files. As we shall see in Section \ref{sec:implement}, the high false positive rate of the detector can make decision-theoretic carving highly inefficient. In a nutshell, SVM-based detector forced our carver to spend a lot of time looking for JPEGs in non-JPEG high-entropy files (ZIP, GIF, RAR files, etc.). As a result, our carver behaved more like a typical sequential carver. The development of efficient and accurate detector of relevant data is, therefore, essential problem in decision-theoretic carving. 

Clearly, an accurate detector of relevant data can be constructed only if the target data has features that reliably distinguishes it from any other data. Even if such features are present, it may not be possible to compute them efficiently\footnote{Consider, for example, the problem of selectively carving child pornography images from unallocated disk space}. Since the aim of this paper was to \emph{demonstrate} viability of the decision-theoretic carving, we developed a simple \emph{ad-hoc} detector for JPEG data that provided acceptable precision and performance characteristics for our experiments (Algorithm \ref{alg:detector}).  

Our detector relies on the presence of escape sequences in JPEG data. Escape sequences are used by JPEG encoder for in-band signalling. When a byte value of 255 is encountered in the JPEG stream, JPEG decoder treats is as the beginning of a command. The subsequent bytes specify what the JPEG decoder must do. For disambiguation purposes any actual 255 data value is encoded as 255 followed by 0.  Our detector defines JPEG data as any high entropy data that contains at least one JPEG escape sequence. Instead of calculating the actual entropy measure, it constructs a histogram of byte values from the given data block and checks that no single value occurs exceedingly often.

\begin{algorithm}[ht]
\begin{algorithmic}
    \\
    \INPUT 
    \\ {array $d[N]$ of $N$ eight-bit bytes.}
    \RETURNS 
    \\ {\underline{true} if $d$ contains JPEG data, \underline{false} otherwise.}
    \VARIABLES 
    \\ {integer array $c[256]$ for counting distinct byte values in $d$, \\ integer variable $esc$ for counting JPEG escape sequences.}
    \\
    \State $esc \gets 0$\Comment{Initialize counters}
    \For{$i=0$ \textbf{to} $255$}
       \State $c[i] \gets 0$
    \EndFor
    \\
    \For{$i=0$ \textbf{to} $N-2$} \Comment{Iterate over $d$}
       \State $val \gets d[i]$
       \State $nextval \gets d[i+1]$
       \\
       \State $c[val] \gets c[val]+1$ 
       \\
       \If{$c[val] > 50$}  

          \Return \underline{false} \Comment{all values should be equally infrequent}
       \EndIf
       \\
       \If{$val = 255$ and $nextval = 255 $}

          \Return \underline{false} \Comment{A 255 must not be followed by 255}
       \EndIf
       \\
       \If{$val = 255$ and $nextval \in \{0,208,209,\dots,215\} $}

          \State $esc \gets esc+1$ 
       \EndIf       
    \EndFor 
    \\
    \If{$esc > 0$}
    
		\Return \underline{true} \Comment{Must contain at least one escape seq.}
    \Else
    
        \Return \underline{false}
    \EndIf       
    
\end{algorithmic}
\caption{Simple detector of JPEG data ($JpegDetect$)}
\label{alg:detector}
\end{algorithm}

Observe that Algorithm \ref{alg:detector} is quite efficient. It has linear time complexity with respect to the size of $d$. This is similar to the complexity of signature-based detection methods used in mainstream file carvers.  

\section{Probability of finding JPEG data in a particular block on the drive}
\label{sec:probability}

The second problem that needs to be solved to create a decision-theoretic JPEG carver is to determine the probability of finding a piece of JPEG digital photograph in a particular data block on the suspect hard disk drive. For the purposes of this article we decided to build a simplified mathematical model of the JPEG carving problem and explore its properties using small-scale simulation. 

We make the following core assumptions about JPEG files:

\begin{enumerate}
\item{JPEG files are not fragmented;}
\item{JPEG files do not overlap and do not share blocks;}
\item{the only other information we have about the target files is the minimal and maximal file size limits specified in the file carver configuration settings.} 
\end{enumerate}

According to the principle of uninformed priors \citep{jaynes2003probability}, we ought make the following additional assumptions:

\begin{enumerate}
\setcounter{enumi}{4}
\item{JPEG file sizes are uniformly distributed between minimal and maximal limits;}
\item{starting locations of JPEG files are uniformly distributed throughout the disk space subject to non-fragmentation and non-overlapping requirements;}
\item{starting locations of JPEG files are uniformly distributed throughout the disk space subject to non-fragmentation and non-overlapping requirements;}
\item{the total number of blocks occupied by relevant data is not known and the probability that the relevant data occupies a certain amount of disk space is uniformly distributed between 0 and 100 percent.}
\end{enumerate}

\subsection{Estimating probability function with small-scale simulation}

To get a sense of the shape and properties of The probability of encountering JPEG  data in a particular block $P(o=r | b_i)$, we performed a series of small-scale simulations using a ``toy'' model of a hard disk drive with only 20 blocks: $B=(b_0,\dots,b_{19})$. The model represents each block with a single value: the value of 1.0 corresponds to blocks containing JPEG data while 0.0 represents blocks with irrelevant information. The small number of blocks in the model allowed brute-force computation of $P(o=r | b_i)$ by constructing all possible ``contents'' of the drive satisfying the assumptions about the location and sizes of JPEG files. The value of  $P(o=r | b_i)$ for a particular block $b_i$ is calculated as the frequency of $b_i$ containing JPEG data across all constructed disk contents\footnote{The corresponding python script \texttt{simulator.py} can be found in the source code of our carver \citep{gladyshevjames2015}.}.

We began our analysis by considering a very simple case: the drive contains single fixed-size JPEG file. The results of the small scale simulation are shown in Figure \ref{fig:fig-20blk-11min-11max2d}. The plot of $P(o=r|b_i)$ has trapezoidal shape. To see why it is so, consider Figure \ref{fig:pa_explan}. Under the assumption of non-fragmentation, an 11-block continuous JPEG file can reside in one of 10 possible positions on a 20-block drive. Observe that two blocks in the middle of the drive are occupied in every possible position of the file, while the very first and the very last blocks on the drive are occupied only in one of ten possible positions of the file. That results in a trapezoidal form of the probability distribution $P(o=r|b_i)$ seen in Figure \ref{fig:fig-20blk-11min-11max2d}.

\begin{figure*}
  \centerline{\includegraphics[width=0.9\textwidth]{fig-20blk-11min-11max2d}}
  \caption{Probability of finding relevant data in a particular disk block, assuming that JPEG files are \emph{exactly} 11 blocks in size.}
  \label{fig:fig-20blk-11min-11max2d}
\end{figure*}

\begin{figure*}
  \centerline{\includegraphics[width=\textwidth]{pa_explain.pdf}}
  \caption{Explanation of the trapezoidal form of the $p(a)$ probability distribution in Figure \ref{fig:fig-20blk-11min-11max2d}.}
  \label{fig:pa_explan}
\end{figure*}

Another case we considered is when the drive contains several non-fragmented files of the same size. Figure \ref{fig:fig-20blk-6min-6max3d} shows the results of the corresponding small scale simulation. In this example we reduced the fixed length of JPEG file down to 6 blocks, and our 20-block drive can now contain up to three such files. For the sake of clarity we added the third dimension to the plot: the total amount of JPEG data blocks stored on the drive. It allows us clearly see the probability $P(o=r|b_i)$ when the drive has either 1, 2 or 3 JPEG files stored on it. To better understand the shapes of these probability plots, the following flashlight analogy could be helpful: imagine an electric flashlight that takes three D-type batteries. A single battery can slide freely inside the flashlight. If we find a flashlight with a single battery inside laying on the floor, the exact position of the battery inside the case is somewhat uncertain to us, which corresponds to the first trapezoidal probability ``ridge'' in Figure \ref{fig:fig-20blk-6min-6max3d}. If, on the other hand, we know that all three batteries are in the flashlight, then we are quite certain where they are inside the flashlight. It corresponds to the last row of three probability ``ridges'' in \ref{fig:fig-20blk-6min-6max3d}, which are considerably higher than the first.

Note also, that if we see someone else's flashlight on the floor and have no prior information about the number of batteries in it, we ought to assume all possibile battery configurations equally likely, and base our decision making on the probability plot that is the average of probability plots for all possible number of batteries inside.

\begin{figure*}
  \centerline{\includegraphics[width=0.9\textwidth]{fig-20blk-6min-6max3d}}
  \caption{Probability of finding relevant data in a particular disk block, given that certain number of blocks are occupied by relevant data and assuming that JPEG files are \emph{exactly} 6 blocks in size.}
  \label{fig:fig-20blk-6min-6max3d}
\end{figure*}

The final case we considered was the drive that contains a number of JPEG files above certain size. The corresponding simulation results are shown in Figure \ref{fig:fig-20blk-6min-20max3d} and Figure \ref{fig:fig-20blk-6min-20max2d}. The latter is the average probability plot assuming that all possible file configurations on the drive are equally likely. 

\begin{figure*}
  \centerline{\includegraphics[width=0.9\textwidth]{fig-20blk-6min-20max3d}}
  \caption{Probability of finding relevant data in a particular disk block, given that certain number of blocks are occupied by relevant data and assuming that JPEG files are \emph{at least} 6 blocks in size.}
  \label{fig:fig-20blk-6min-20max3d}
\end{figure*}

\begin{figure*}
  \centerline{\includegraphics[width=0.9\textwidth]{fig-20blk-6min-20max2d}}
  \caption{Probability of finding relevant data in a particular disk block, assuming that JPEG files are \emph{at least} 6 blocks in size.}
  \label{fig:fig-20blk-6min-20max2d}
\end{figure*}

A notable feature of the Figure \ref{fig:fig-20blk-6min-20max2d} is the presence of two peaks that make it look somewhat like a cat's head. Each of the peaks is located at the offset corresponding to the minimal file length away from each end of the drive. These peaks offer a slightly higher probability of finding JPEG data there, than on the rest of the drive, and -- if $\frac{1}{T_{proc}(a)}$ is constant -- these peaks are the optimal choices for the decision-theoretic block testing according to the Equation (\ref{eq:carving-decision}).

\subsection{Simulating results of successful and unsuccessful block testing}

Our model can also be used to explore the result of testing a particular block on the drive and finding either relevant or irrelevant data there. This is achieved by restricting probability calculation to only those drive configurations that either do or do not contain relevant data in the tested block. 

Figure \ref{fig:fig-20blk-6min-20max-testloc-5-0-2d} shows the result of testing the drive block $b_5$ corresponding to the first probability peak in Figure \ref{fig:fig-20blk-6min-20max2d} and finding no JPEG data there. Observe that the negative test at $b_5$ implies that blocks $b_0 \dots b_4$ also do not contain JPEG data. It is the consequence of assuming a lower bound of 6 blocks on JPEG file size together with the absence of fragmentation. 

\begin{figure*}
  \centerline{\includegraphics[width=0.9\textwidth]{fig-20blk-6min-20max-testloc-5-0-2d}}
  \caption{Posterior probability of finding JPEG data in a particular disk block, after testing $b_5$ and finding no relevant JPEG data there. All JPEG files are assumed to be at least 6 blocks in length.}
  \label{fig:fig-20blk-6min-20max-testloc-5-0-2d}
\end{figure*}

Note also that the shape of the posterior probability distribution to the right of $b_5$ is quite similar to the form of the initial probability distribution from Figure \ref{fig:fig-20blk-6min-20max2d}. The difference is that the first peak of probability is now at the block $b_{11}$ instead of $b_{5}$. This is again a direct consequence of the non-fragmentation assumption and the minimal file size assumption. 

The implication of the Figure \ref{fig:fig-20blk-6min-20max-testloc-5-0-2d} is that we may not always need to check every block on the drive to find all JPEGs above certain size. In the small-scale model from Figure \ref{fig:fig-20blk-6min-20max-testloc-5-0-2d} it suffices to check only three blocks: $b_5$,$b_{11}$, and $b_{14}$ to establish that there is no JPEGs on the 20-block drive! The larger the minimal size of JPEG files, the less blocks we need to check.

Figure \ref{fig:fig-20blk-6min-20max-testloc-5-1-2d} shows the result of testing drive block $b_5$ and finding that it contains relevant data. As expected, positive test increases the probability of finding JPEG data at $b_5$ to $1.0$. Note that the next most likely blocks to contain JPEG data are blocks $b_4$ and $b_6$ adjacent to $b_5$. This result is in full accordance with the human intuition of how to play the Battleships game: once an enemy ship is discovered by random bombing, human player finishes it by bombing squares adjacent to the first successful hit.

\begin{figure*}
  \centerline{\includegraphics[width=0.9\textwidth]{fig-20blk-6min-20max-testloc-5-1-2d}}
  \caption{Posterior probability of finding JPEG data in a particular disk block, after testing $b_5$ and finding that it contains JPEG data. All JPEG files are assumed to be at least 6 blocks in length.}
  \label{fig:fig-20blk-6min-20max-testloc-5-1-2d}
\end{figure*}

\section{Exploring properties of $T_{proc}(a)$ for hard disk drives and solid state drives.}
\label{sec:tproc}

The final piece of knowledge that needs to be acquired in order to build a decision-theoretic carver is the form and properties of $T_{proc}(a)$. In order to better understand its properties for actual HDD and SSD devices, we performed a series of experiments on a 120Gb Fujitsu HDD and a 120Gb SSDNow SSD. 

Two types of experiments were performed: one group of experiments measured $T_{proc}(a)$ for random block access, and the other group of experiments measured sequential access time. Given below is the summary of our findings.

To measure the time required for random block access, 1,000 random block tests were performed for each drive. Each random block test consists of a drive seek operation followed by reading the target disk block and performing JPEG data detection on the read data using Algorithm \ref{alg:detector}. The time required to complete each experiment was measured. We denote this time as $T_{rbt}(a)$. Figures \ref{fig:seek_time_hdd} and \ref{fig:seek_time_ssd} show the empirical $T_{rbt}(a)$ values for the 120Gb HDD and 120Gb SSD respectively. 

\begin{figure*}
  \centerline{\includegraphics[width=0.9\textwidth]{seek-time-hdd}}
  \caption{Time of random block testing on Fujitsu 120 Gb HDD.}
  \label{fig:seek_time_hdd}
\end{figure*}

\begin{figure*}
  \centerline{\includegraphics[width=0.9\textwidth]{seek-time-ssd}}
  \caption{Time of random block testing on SSDNow 120 Gb SSD.}
  \label{fig:seek_time_ssd}
\end{figure*}

The experimental data is quite noisy, which is probably due to RAM caching of the previously read blocks and fluctuations of normal system activity. Nevertheless, it suggests linear relationship between the absolute seek distance $|a|$ and processing time:

\begin{equation} \label{eq:tproc-eq}
T_{rbt}(a)=\beta |a|+\gamma
\end{equation}

where $\beta$ and $\gamma$ can be estimated using linear regression of the experimental data. The resulting linear approximations are shown in Figures \ref{fig:seek_time_hdd} and \ref{fig:seek_time_ssd} as dashed lines. We observe that in both cases $T_{rbt}(a)$ has relatively large constant component($\gamma$) and very shallow slope ($\beta$). In SSD experiments $\beta$ is almost zero and $T_{rbt}(a)$ is essentially a constant over the entire range of seek offsets. In HDD experiments $T_{rbt}(a)$ clearly increases proportionally to the seek offset, but despite a pronounced visual appearance in Figure \ref{fig:seek_time_hdd}, the sloping angle of $T_{rbt}(a)$ is actually so shallow and that it can be ignored at short travel distances like the size of a large JPEG file (see footnote\footnote{According to the estimates shown in Figure \ref{fig:seek_time_hdd}, HDD seek time increases from around 15 ms for short jumps to aproximately 32 ms for jumps across the entire 120 Gb of the HDD. Most JPEGs are under 50Mb in size. At such small distances the increase of $T_{rbt}(a)$ would be only 0.007 ms or 0.0005\% of the constant 15 ms. Thus, for short jumps comparable to the size of JPEG file, we can assume $T_{rbt}(a)$ to be constant.} for explanation).

Although random block access is important for decision-theoretic carving, we should not overlook properties of sequential block access on SSD and HDD. It is well known that both SSDs and HDDs are optimised for sequential block access, and it is much quicker to read 100 blocks sequentially rather than randomly. Suppose that our file carver needs to examine content of a block at a (positive) offset $a$ from the last examined block. This can be done either by a direct jump followed by the block processing or by reading and processing consecutive blocks up to and including the target block. To distinguish the two approaches we denote the time required to read and examine $a$ consecutive blocks sequentially as $T_{seq}(a)$.

To determine how these approaches compare, we conducted additional experiments measuring the time taken to read and apply signature matching to large stretches of consecutive blocks on the 120Gb Fujitsu HDD and the 120Gb SSDNow SSD. As expected, $T_{seq}(a)$ grows at a rate proportional to $a$ for both SSD and HDD. To compare the behaviour of $T_{rbt}(a)$ and $T_{seq}(a)$, we plotted them together for the SSD (Figure \ref{fig:processing_time_ssd}) and HDD (Figure \ref{fig:processing_time_hdd}) respectively. 

\begin{figure*}
  \centerline{\includegraphics[width=0.9\textwidth]{processing-time-ssd}}
  \caption{$T_{rbt}(a)$ and $T_{seq}(a)$ calculated for SSDNow 120 Gb SSD.}
  \label{fig:processing_time_ssd}
\end{figure*}

\begin{figure*}
  \centerline{\includegraphics[width=0.9\textwidth]{processing-time-hdd}}
  \caption{$T_{rbt}(a)$ and $T_{seq}(a)$ calculated for Fujitsu 120 Gb HDD.}
  \label{fig:processing_time_hdd}
\end{figure*}

It is easy to see that $T_{seq}(a)$ is smaller for small values of $a$, while $T_{rbt}(a)$ is smaller for large values of $a$. The point of equivalence is $e$ where

\begin{equation}
  T_{rbt}(e)=T_{seq}(e)
\end{equation}

It is hardware-dependent and in our experiments it varied between $10^2$ and $10^3$ 4Kb blocks for SSDs and between $10^4$ and $10^5$ 4Kb blocks for HDDs.

\section{Designing new carving strategy for large, non-fragmented JPEG files}
\label{sec:decision}

Based on the results of Sections \ref{sec:detector}, \ref{sec:probability} and \ref{sec:tproc} we are now in a position to propose a new strategy for carving large non-fragmented JPEG files.

We first go back to the decision equation (\ref{eq:carving-decision}) and substitute the right-hand side of \ref{eq:tproc-eq} for $T_{proc}(a)$ in it: 

\begin{equation} \label{eq:carving-decision-2}
a = \argmax_{a_i \in A} \left( \frac{1}{\beta a_i + \gamma} p(a_i,l)\right)
\end{equation}

As demonstrated in the Figure \ref{fig:seek_time_ssd}, SSDs have constant $T_{proc}(a_i)$. It effectively turns $\frac{1}{\beta a_i + \gamma}$ into a constant scaling factor $\frac{1}{\gamma}$, and the shape of the utility expression is determined entirely by the shape of $p(a_i,l)$.

For HDDs the effect of $T_{proc}$ on the decision making is more pronounced. Since $T_{proc}$ increases proportionally to the jump distance $a_i$, the multiplier $\frac{1}{\beta a_i + \gamma}$ decreases inversely proportional to $a_i$. The net effect of this is that out of two blocks with equal $p(a_i,l)$,  Equation (\ref{eq:carving-decision}) would favour the block closest to the previously examined block. Due to extremely shallow slope, however, the effect of utility reduction is significant only at values of $a_i$ that are many times greater than the size of typical JPEG file. At values of $a_i$ comparable with the size of a typical JPEG file, we can assume $\frac{1}{\beta a_i + \gamma}$ to be constant, and the shape of the utility expression in Equation (\ref{eq:carving-decision-2}) determined entirely by the shape of $p(a_i,l)$ as in the SSD case.

The results presented in Section \ref{sec:probability}
indicate that the probability function $p(a_i,l)$ has two peaks: one at the distance $L_{min}-1$ from the current position on the drive, where $L_{min}$ is the minimal size of JPEG file in blocks, and the other peak is at the distance $L_{max}-(L_{min}-1)$, where $L_{max}$ is the size of the unexplored portion of data blocks adjacent to the last examined block. For SSDs both locations are \emph{equally} optimal, because $T_{proc}$ is constant, but for HDDs the location $L_{max}-(L_{min}-1)$ is clearly sub-optimal, if $L_{max}$ is large. In either case, \emph{choosing to examine disk block at the offset $L_{min}-1$ blocks from the last examined block seems to be the optimal decision when either initiating the carving process or after an unsuccessful block test.}

It is easy to see that for relatively small JPEG files, whose sizes are below the intersection point $e$ of $T_{rbt}(a)$ and $T_{seq}(a)$, sequential carving \emph{is} the optimal strategy\footnote{Note that while sequential carving is optimal for recovering \emph{all} small files, it may not always be necessary to recover all of them. In triage situations we may be content with just a subset of JPEGs, in which case random access testing may still produce a speed-up.}, because $T_{seq}(a)$ is the fastest way to ``sample'' and extract all blocks at $a_i$ distances equal to the size of such files. For solid state drives these are files less than 1-2 Mb in size, while for hard disk drives it includes virtually all JPEG files.

Once JPEG data is detected through the block testing, the file carver needs to determine the boundaries of the detected JPEG file. Figure \ref{fig:fig-20blk-6min-20max-testloc-5-1-2d} suggests that in absence of drive seek delay, the optimal approach would have been to examine blocks immediately adjacent to the detected block using, for example, signature based JPEG header and footer detection. 

In our experiments only the forward directed sequential block reading did not incur drive-seek penalty. The reverse sequential block reading exhibited timing characteristics comparable with random block access.  Thus, while sequential carving is optimal for finding JPEG footer, a combination of sequential and random access block reading (e.g. based on binary search) may have to be employed to achieve optimal performance. We note however, that the minimal sizes of high resolution JPEG files are not much bigger than the point of equivalence $e$ between $T_{rbt}(a)$ and $T_{seq}(a)$, and having more that one drive seek operation while searching for JPEG header can be less efficient than sequential carving. For the purposes of this article, we chose to adopt a somewhat simplistic approach that we call \emph{Bounded Sequential Carving}: once our carver detects JPEG data in a sampled block, it performs a single backward jump followed by the usual sequential carving to find the header and footer of the detected drive. We leave analysis of more advanced strategies as a topic for future research.

\section{DECA: a decision-theoretic carving program}

The analysis given in the preceding sections led us to the file carving algorithm that combines data sampling with sequential carving. It is designed for fast carving of large non-fragmented JPEGs like high resolution pictures produced by DSLR cameras. We named it \emph{DECA} that stands for decision-theoretic carving. 

DECA operates on a block device viewed as an array of data blocks $B$ \footnote{Note that in addition to actual raw block devices, DECA could be applied to partitions, forensic disk images, etc.}. The user has to specify the minimal size of expected JPEG files $l$. 

DECA alternates between two modes: \emph{data sampling} and \emph{bounded sequential carving}. It starts in the data sampling mode (Algorithm \ref{alg:deca-js}). DECA skips the first $l-1$ blocks of $B$ by jumping directly to the block $b_{l-1}$ and reading its content. DECA then runs the Algorithm \ref{alg:detector} to check for presence of JPEG data in the block. If no JPEG data is detected, DECA skips the following $l$ blocks and continues sampling at the block $b_{2l-1}$, but if JPEG data \emph{is} detected at the block $b_{l-1}$, DECA jumps backwards $l-1$ blocks (to the block $b_0$) and switches to the bounded sequential carving. 

The bounded sequential carving mode (Algorithm \ref{alg:deca-bsc}) is broadly similar to ``normal'' sequential carving. It reads consecutive data blocks and detects JPEG files using header and footer signatures. The main difference is that it reverts back to the data sampling mode when the JPEG header signature is not found after $l$ blocks of searching

Note that DECA does not revert immediately to the sampling mode after finding the JPEG footer. It keeps searching for the next JPEG header for $l$ more blocks. This is important, because humans tend to copy or move JPEG files in groups, as and such JPEG files tend to be stored back-to-back (or within a short distance from each other) on the block device. 

\begin{algorithm}[ht]
\begin{algorithmic}
    \\
    \INPUT 
    \\ {array of $m+1$ data blocks $B = \left(b_0,\cdots,b_m\right)$;}
    \\ {minimal JPEG file length in blocks $l$, where $0 \le l \leq (m+1)$.}
    \VARIABLES
    \\ {index of the next block to be examined $pos$}
    \\ {contents of the next block to be examined $d$}
    \\
    \State $pos \gets (l-1)$
    \While{$pos \leq m$}
       \State $JumpTo(b_{pos})$
       \State $d \gets Read(b_{pos})$
       \If{$JpegDetect(d)$}  
          \State $pos \gets pos - (l-1)$
          \State $pos \gets BSC(B,pos,l)$ \Comment{Bounded seq. carving}
          \State $pos \gets pos + (l-1)$
       \Else
          \State $pos \gets pos + l$
       \EndIf 
    \EndWhile 
\end{algorithmic}
\caption{DECA: Data sampling mode.}
\label{alg:deca-js}
\end{algorithm}

\begin{algorithm}[ht]
\begin{algorithmic}
    \\
    \INPUT
    \\ {array of $m+1$ data blocks $B = \left(b_0,\cdots,b_m\right)$;}
    \\ {index of the starting data block $start$;}
    \\ {JPEG header search bound (in blocks) $l$, where $0 \le l$;}
    \VARIABLES
    \\ {index of the next block to be examined $pos$}
    \\ {contents of the next block to be examined $d$}
    \\ {index of the first block of JPEG file $jpegpos$}
    \\ {size (in blocks) of the JPEG file $size$}
    \\ {counter of remaining blocks within bound $bound$}
    \CONSTANTS
    \\ {maximal allowed size of JPEG file (in blocks) $M$}
    \RETURNS
    \\ {position of the last examined block ($pos$)}
    \\
    \State $pos \gets start$
    \State $bound \gets l$
    \State $JumpTo(b_{pos})$ 
    \While{$pos \leq m$ AND $bound > 0$} 
       \State $d \gets read(b_{pos})$
       \If{$Header(d)$} 
 	      \State $jpegpos \gets pos$
    	  \State $size \gets 1$
    	  \While{$\neg Footer(d)$ AND $size < M$ AND $pos < m$}
    	     \State $pos \gets pos+1$
    	     \State $size \gets size+1$
    	  	 \State $d \gets read(b_{pos})$
    	  \EndWhile
    	  \State $ExtractDataBlocks(b_{jpegpos},\cdots,b_{pos})$
    	  \State $pos \gets pos+1$
    	  \State $bound \gets l$  
       \Else
         \State $bound \gets bound-1$
       \EndIf
    \EndWhile
	\\
	\Return {$pos$}      
    
\end{algorithmic}
\caption{DECA: Bounded sequential carving ($BSC$).}
\label{alg:deca-bsc}
\end{algorithm}

\section{Implementation and Evaluation of DECA carver} \label{sec:implement}
The previously described DECA algorithm was implemented using C++. The source code can be found in \citep{gladyshevjames2015}. The software implementation defaults to linear carving mode. Two of the main functions of the software. First, a disk can be sampled to determine its seek time. With seek-time information, an investigator can determine jump distance at which DECA carving becomes faster than traditional linear carving. Second, the software allows for carving using the DECA algorithm while setting the minimal sector jump distance (in 512 sector increments).

Experimentation seeks to demonstrate the theorized performance of the DECA algorithm, and compare the algorithm to other file carvers commonly used in digital investigations.

\subsection{Experimentation Design}
DECA is based on estimates of the distribution of JPEG data on a disk. Based on this estimation, DECA attempts to search only the most likely locations for related data. The result of the DECA algorithm is that some sectors will be skipped. In some cases, skipping sectors that are unlikely to contain relevant data may result in carving speed up. However, this method is also likely to miss smaller files.

DECA, in practical terms, can be described simply. If the jump size is high, carve times and number of carved files will decrease. If the jump size is low, carve times will increase and the number of carved files will increase. As discussed, this depends on the seek time of the disk holding the data to be analyzed. From the prior description, we propose that DECA is most beneficial when attempting to carve a small number of large files from a disk with a low seek time.

From the description of DECA we propose the following hypotheses for testing:
\begin{enumerate}
	\item DECA will have a more significant reduction in run-time as the seek time of the disk decreases compared to other carving methods.
	\item DECA will have a more significant reduction in run-time compared to other carving methods based on the size of disk/image being analyzed.
	\item DECA will normally find fewer files compared to linear carvers.
	\item DECA is best suited for carving a small number of large files on a low-seek-time disk.
\end{enumerate}

\subsection{Experiment 1}
Two disk images were created for testing. One is based on a 4GB USB thumb drive, with a collection of JPEG images along with other documents stored in a FAT32 partition. The second is a 32GB image of a Windows XP operating system containing a user-created documents and JPEG images taken with various digital cameras.

Along with DECA, other carvers tested with the same data sets include Photorec, Scalpel 1.60, Scalpel 2.1 and Foremost. Each of these carvers were configured to only search for JPEG images.

\subsection{Evaluation}
To test hypothesis 1, each carver was used on the test data set while stored on an sold state drive (SSD) and hard disk drive (HDD). The 4GB image was also tested inside a RAMDisk. RAM cache was cleared before each run. Measurement of carving performance is based on sampling the number of carved files in the carver's output directory (contained on the same disk as the test image) every 0.25 seconds until the carver terminates. All raw test data and testing scripts can be found in the project repository given above.

To determine whether DECA had a reduction in run-time compared to linear carving, we compared the average speed increase over all linear carvers (including DECA in linear carving mode) to the average speed increase between DECA with 7 minimum-block-size configurations (default, 64, 200, 300, 600, 1200, 2400).

The average speed increase for linear carving from HDD to SSD was 237\%. The average speed increase for DECA carving from HDD to SSD was 553\%. The average speed increase for linear carving from SSD to RAMDisk was 327\%. The average speed increase for DECA carving from SSD to RAMDisk was 1229\%. Overall, the average speed increase for linear carving was 298\%, while the average speed increase for DECA carving was 891\%. As expected, when DECA had lower minimum file-size values (64 to 300), performance was equal to or below linear carving on disks with high seek times. Hypothesis 1 is found to be true in most cases except when DECA minimum-file-size is less than 600 on high seek-time disks. The disk seek-time will affect the level of minimum-file-size that should be used.

%\begin{figure}
%	\includegraphics[width=\textwidth]{figures/HDDsmall.png}
%	\caption{DECA carving compared to popular linear carvers where the x-axis is the number of images carved; the y-axis is the number of samples in 0.25 increments; the length of the line is the total time of the carving process on a hard disk drive with a high seek-time. Here DECA carving has similar run times while the minimum-file-size is \textless 600.}
%	\label{fig:HDDsmallAnalysis}
%\end{figure}
%
%\begin{figure}
%	\includegraphics[width=\textwidth]{figures/RAMDISKsmall.png}
%	\caption{DECA carving compared to popular linear carvers where the x-axis is the number of images carved; the y-axis is the number of samples in 0.25 increments; the length of the line is the total time of the carving process on a RAMDisk drive with a low seek-time.}
%	\label{fig:RAMDISKsmallAnalysis}
%\end{figure}

To test hypothesis 2 we calculated the average speed increase for linear carving and DECA carving from HDD to SSD. For the 4GB image, DECA performed 316\% over the linear carver average. For the 32GB image DECA performed 434\% over the linear carver average. Similar to before, if DECAs minimum-file-size was below 600, DECA performed at or below the linear carver average for the 32GB drive. As can be seen in Figure \ref{fig:4v32}, DECA saw greater performance increases from HDD to SSD with a larger disk image when minimum-file-size is greater than 600. This is explained by the cumulative effect of continually skipping large sections of the disk. 

\begin{figure*}
	\includegraphics[width=\textwidth]{figures/4GBv32GB.png}
	\caption{DECA time performance improvements in percentage when comparing carving times on an HDD to carving times on an SSD. This graph shows that DECA performs better with larger disk images when minimum-file-size is greater than 600.}
	\label{fig:4v32}
\end{figure*}

To test hypothesis 3 we counted the average number of files returned from each carving process. On the 4GB disk image, linear carving returned 356 files on average, and DECA carving returned 30 files on average (regardless of underlying disk). This is consistent with what was predicted since DECA is likely to skip small files and fragments that linear carving will find.

To test hypothesis 4, we examine results of the largest tested DECA minimum-file-size. In our experiments, the largest minimum-file-size used was 2400, which tells DECA that a file is likely to be located within 1,200Kb of its current location. With this setting, DECA was 5 times faster than the linear carving average on an SSD, and 19 times faster than the linear carving average on a RAMDisk. However, DECA only recovered 2\% of the files recovered by linear carving. Overall, it appears that hypothesis 3 is also true. By expanding the minimum-file-size DECA accepts and using a disk with a lower seek-time, DECAs speed will dramatically increase at the expensive of smaller file recovery.

\subsubsection{Discussion}
The DECA algorithm worked as theorized, however, a practical implementation is likely more interesting to investigators. DECA produces significant carving speed increases under certain circumstances, but always at the cost to the number of files carved. Specifically, DECA is ideal for carving a small number of large files across a large, low-seek-time disk. For this reason, DECA may be appropriate for triage purposes, where very fast but incomplete image recovery is acceptable. In cases where the maximum number of files should be carved, or when carving high-seek-time devices (HDD), linear carving largely outperforms DECA.

As discussed, DECA jumps to sections of the disk based on a calculated data distribution. DECA then checks for an image header or footer. If not found, then DECA attempts to classify the data as a JPEG or not. The classifier that was developed could likely be improved. This would increase the number of images correctly recovered, potentially at the cost of speed.

The current implementation of DECA focuses on JPEG images, while imagining current digital cameras that produce images greater than 1 megabyte. However, this algorithm may be suitable for other specific file types, such as music, video, zip files and email containers. This method could also be used in conjunction with other carving methods, such as in-place carving \citep{Richard2007inplace, MeijerRob2012}.

\section{Conclusions}
Decision theoretic analysis allows a file carver to consider the most likely locations of relevant data based on what is currently known about the distribution of data on the disk. By skipping sections of the disk that are unlikely to contain relevant data, carving times can be greatly decreased, where speed benefits are cumulative as the disk becomes larger. Through a practical implementation we demonstrated that decision theoretic analysis is useful when carving a small number of files from a large, low-seek-time disk. With current storage disk technology, decision theoretic carving (DECA) is best implemented as a triage solution. For traditional carving needs, linear carving gives more comprehensive results, and on standard hard disk drives, also has similar carving speeds.

\subsection{Future Work}
This work is a first effort to propose decision theoretic analysis applied to file carving. There are currently a number of limitations to the practical implementation (DECA) carver. Specifically, file distribution has been pre-calculated based on surveyed data in other drives. Future work will attempt to find a method for calculating a data distribution on the disk to be analyzed without increasing carving time significantly. Next, as DECA jumps to a location on the disk it must detect whether the data stored in that location is relevant. We have attempted several detection methods, most of which significantly increased the overall run time. We will seek to improve the data classifier that DECA uses. 




\bibliographystyle{elsarticle-harv} 
\bibliography{DECA}

\end{document}

\endinput
%%
%% End of file `elsarticle-template-harv.tex'.
